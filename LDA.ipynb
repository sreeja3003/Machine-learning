{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the LDA modeled data looks likes:\n",
      "[(0, '0.051*\"must\" + 0.051*\"fire\" + 0.051*\"defam\" + 0.051*\"wit\"'), (1, '0.054*\"nz\" + 0.054*\"air\" + 0.054*\"strike\" + 0.033*\"pay\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "d_1 = \"aba decides against community broadcasting licence.\"\n",
    "d_2 = \"act fire witnesses must be aware of defamation.\"\n",
    "d_3 = \"a g calls for infrastructure protection summit.\"\n",
    "d_4 = \"air nz staff in aust strike for pay rise.\"\n",
    "d_5 = \"air nz strike to affect australian travellers.\" \n",
    "d_6 = \"ambitious olsson wins triple jump.\" \n",
    "d_7 = \"antic delighted with record breaking barca.\"\n",
    "d_8 = \" .\"\n",
    "suit = [d_1, d_2, d_3, d_4, d_5,d_6,d_7,d_8]\n",
    "texts = []\n",
    "for i in suit:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "print(\"the LDA modeled data looks likes:\")\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Applications/anaconda3/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Applications/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Applications/anaconda3/lib/python3.8/site-packages (from gensim) (1.18.5)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=108249 sha256=75f0d26c0d200e2a6792989d8027ee630a41d49ac75ab49ffbef866b92bff7f3\n",
      "  Stored in directory: /Users/sreejamadanambeti/Library/Caches/pip/wheels/8c/f9/f4/4ddd9ddee3488f48be20e9bf3108961f03ae23da29b7ed26d1\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "Building wheels for collected packages: stop-words\n",
      "  Building wheel for stop-words (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32916 sha256=de7fec1384317b359b2aca32f1f4f6cd2fdcd842f7a38666d2e31c75fb4f7061\n",
      "  Stored in directory: /Users/sreejamadanambeti/Library/Caches/pip/wheels/eb/03/0d/3bd31c983789aeb0b4d5e2ca48590288d9db1586cf5f225062\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2018.7.23\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
